{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import pickle\n",
        "import pretty_midi\n",
        "import pt_util\n",
        "import os\n",
        "from os import listdir\n",
        "from os.path import join, isfile\n",
        "import threading\n",
        "import tqdm\n",
        "import random\n",
        "\n",
        "print('Version', torch.__version__)\n",
        "print('CUDA enabled:', torch.cuda.is_available())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4IWxgzIEe6qc"
      },
      "outputs": [],
      "source": [
        "BASE_PATH = '/Users/markusschiffer/music-generation'\n",
        "if not os.path.exists(BASE_PATH):\n",
        "    os.makedirs(BASE_PATH)\n",
        "DATA_PATH = 'data\\\\'\n",
        "os.chdir(BASE_PATH)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gU1IV06Gm_2z"
      },
      "outputs": [],
      "source": [
        "NUM_THREADS = 4\n",
        "\n",
        "def add_note(ind2note, note2ind, note):\n",
        "    if note not in note2ind:\n",
        "        note2ind[note] = len(note2ind)\n",
        "        ind2note[len(ind2note)] = note\n",
        "    return note2ind[note]\n",
        "\n",
        "def add_tokenized(tokenized_list, tokenized):\n",
        "    tokenized_list.append(tokenized)    \n",
        "\n",
        "def thread_task(note2ind, ind2note, tokenized_musics, dict_lock, list_lock, workload_list, thread_num, fs):\n",
        "    \"\"\"\n",
        "    task for thread\n",
        "    \"\"\"\n",
        "    for i, midi_file in enumerate(workload_list):\n",
        "        if i % 25 == 0:\n",
        "            print(f'thread {thread_num} processed {i} files')\n",
        "        try:  # Handle exception on malformat MIDI files\n",
        "            midi_pretty_format = pretty_midi.PrettyMIDI(midi_file)\n",
        "            piano_midi = midi_pretty_format.instruments[0]  # Get the piano channels\n",
        "            # piano_roll (each row is a pitch/note, each col is an array [played notes])\n",
        "            piano_roll = piano_midi.get_piano_roll(fs=fs)\n",
        "            # can we assume times is always in a descending order?\n",
        "            index = np.where(piano_roll > 0)\n",
        "            times = np.unique(index[1])\n",
        "            # index[0] is notes, index[1] is timeframes\n",
        "            cur_music = np.zeros(piano_roll.shape[1], dtype=np.int64)\n",
        "            for time in times:\n",
        "                index_where = np.where(index[1] == time)\n",
        "                notes = index[0][index_where]\n",
        "                notes = tuple(notes)\n",
        "                if notes in note2ind: # same note at different tf\n",
        "                    cur_music[time] = note2ind[notes]\n",
        "                else:\n",
        "                    dict_lock.acquire()\n",
        "                    cur_music[time] = add_note(ind2note, note2ind, notes)\n",
        "                    dict_lock.release()\n",
        "            list_lock.acquire()\n",
        "            add_tokenized(tokenized_musics, np.trim_zeros(cur_music))\n",
        "            list_lock.release()\n",
        "        except Exception as e:\n",
        "            # locks should not be aquired if exception were to occur\n",
        "            print(e)\n",
        "            print(\"broken file : {}\".format(midi_file))\n",
        "\n",
        "def prepare_data(data_path, fs, pickle_name, all_maestro=False, remove_empty=False):\n",
        "    # global vars\n",
        "    note2ind = {}\n",
        "    ind2note = {}\n",
        "    tokenized_musics = []\n",
        "\n",
        "    # locks\n",
        "    dict_lock = threading.Lock()\n",
        "    list_lock = threading.Lock()\n",
        "\n",
        "    if all_maestro:\n",
        "        midi_files = []\n",
        "        dates = [\"2004\", \"2006\", \"2008\", \"2009\", \"2011\", \"2013\", \"2014\", \"2015\", \"2017\", \"2018\"]\n",
        "        for date in dates:\n",
        "            midi_files.extend([join(data_path, date, f) for f in listdir(join(data_path, date))])\n",
        "    else:\n",
        "        midi_files = [join(data_path, f) for f in listdir(data_path)]\n",
        "    k, m = divmod(len(midi_files), NUM_THREADS)\n",
        "    workloads = list(midi_files[i*k+min(i, m):(i+1)*k+min(i+1, m)] for i in range(NUM_THREADS))\n",
        "    note2ind = {}\n",
        "    ind2note = {}\n",
        "    empty_note = ()\n",
        "    note2ind[empty_note] = 0\n",
        "    ind2note[0] = empty_note\n",
        "    # a list of np arrays, where the size of each np array is the length of the song\n",
        "    tokenized_musics = []\n",
        "    \n",
        "    # creating threads\n",
        "    threads = [threading.Thread(target=thread_task, args=(note2ind, ind2note, tokenized_musics, dict_lock, list_lock, workloads[j], j, fs)) for j in range(NUM_THREADS)]\n",
        "\n",
        "    # start threads\n",
        "    for thread in threads:\n",
        "        thread.start()\n",
        "\n",
        "    # wait until threads finish their job\n",
        "    for thread in threads:\n",
        "        thread.join()\n",
        "\n",
        "    tokenized = np.concatenate(tokenized_musics, axis=0)\n",
        "    train_text = tokenized[:int(0.8 * len(tokenized))]\n",
        "    test_text = tokenized[int(0.8 * len(tokenized)):]\n",
        "\n",
        "    pickle.dump({'tokens': train_text, 'ind2note': ind2note, 'note2ind':note2ind}, open(DATA_PATH + f\"music_train_{pickle_name}_{fs}.pkl\", \"wb\"))\n",
        "    pickle.dump({'tokens': test_text, 'ind2note': ind2note, 'note2ind':note2ind}, open(DATA_PATH + f\"music_test_{pickle_name}_{fs}.pkl\", \"wb\"))\n",
        "\n",
        "prepare_data(\"musics/Schumann\", 2, \"schumann_2\", all_maestro=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class Vocabulary(object):\n",
        "    def __init__(self, data_file):\n",
        "        with open(data_file, 'rb') as data_file:\n",
        "            dataset = pickle.load(data_file)\n",
        "        self.ind2note = dataset['ind2note']\n",
        "        self.note2ind = dataset['note2ind']\n",
        "\n",
        "    # Returns a MIDI notes representation of the tokens.\n",
        "    def array_to_notes(self, arr):\n",
        "        return [self.ind2note[int(ind.item())] for ind in arr]\n",
        "\n",
        "\n",
        "    # Returns a torch tensor representing each token in notes.\n",
        "    def notes_to_array(self, notes):\n",
        "        return torch.LongTensor([self.note2ind[note] for note in notes])\n",
        "\n",
        "    # Returns the size of the vocabulary.\n",
        "    def __len__(self):\n",
        "        return len(self.note2ind)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n92Pi9p2jTZN"
      },
      "outputs": [],
      "source": [
        "class PianoDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, data_file, sequence_length, batch_size):\n",
        "        super(PianoDataset, self).__init__()\n",
        "\n",
        "        self.sequence_length = sequence_length\n",
        "        self.batch_size = batch_size\n",
        "        self.vocab = Vocabulary(data_file)\n",
        "        self.data = []\n",
        "\n",
        "        with open(data_file, 'rb') as data_pkl:\n",
        "            dataset = pickle.load(data_pkl)\n",
        "        remain = len(dataset['tokens']) % batch_size\n",
        "        if remain != 0:\n",
        "          dataset['tokens'] = dataset['tokens'][:-remain]\n",
        "\n",
        "        chunk_ranges = []\n",
        "        chunk_size = len(dataset['tokens']) // batch_size\n",
        "        for i in range(0, len(dataset['tokens']), chunk_size):\n",
        "            chunk_ranges.append((i, i + chunk_size - 1))\n",
        "        for i in range(0, chunk_size, self.sequence_length):\n",
        "            for chunk in chunk_ranges:\n",
        "                start = i + chunk[0]\n",
        "                if not start + self.sequence_length + 1 > chunk[1]:\n",
        "                    self.data.append((torch.LongTensor(dataset['tokens'][start:start+self.sequence_length]), torch.LongTensor(dataset['tokens'][start+1:start+self.sequence_length+1])))\n",
        "                else:\n",
        "                    self.data.append((torch.LongTensor(dataset['tokens'][start:chunk[1]]), torch.LongTensor(dataset['tokens'][start+1:chunk[1]+1])))\n",
        "\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "        \n",
        "    def __getitem__(self, idx):\n",
        "        # Return the data and label for a character sequence as described above.\n",
        "        # The data and labels should be torch long tensors.\n",
        "        # You should return a single entry for the batch using the idx to decide which chunk you are \n",
        "        # in and how far down in the chunk you are.\n",
        "        \n",
        "        return self.data[idx]\n",
        "\n",
        "    def vocab_size(self):\n",
        "        return len(self.vocab)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5nTjVZBRadc-"
      },
      "outputs": [],
      "source": [
        "class PianoLSTMNet(nn.Module):\n",
        "    def __init__(self, vocab_size, feature_size):\n",
        "        super(PianoLSTMNet, self).__init__()\n",
        "        self.vocab_size = vocab_size\n",
        "        self.feature_size = feature_size\n",
        "        self.encoder = nn.Embedding(self.vocab_size, self.feature_size)\n",
        "        self.lstm = nn.LSTM(self.feature_size, self.feature_size, batch_first=True, num_layers=2)\n",
        "        self.decoder = nn.Linear(self.feature_size, self.vocab_size)\n",
        "        \n",
        "        # This shares the encoder and decoder weights as described in lecture.\n",
        "        self.decoder.weight = self.encoder.weight\n",
        "        self.decoder.bias.data.zero_()\n",
        "        \n",
        "        self.best_accuracy = -1\n",
        "    \n",
        "    def forward(self, x, hidden_state=None):\n",
        "        # batch_size = x.shape[0]\n",
        "        # sequence_length = x.shape[1]\n",
        "\n",
        "        x = self.encoder(x)\n",
        "        x, hidden_state = self.lstm(x, hidden_state)\n",
        "        x = self.decoder(x)\n",
        "         \n",
        "\n",
        "        return x, hidden_state\n",
        "\n",
        "    # This defines the function that gives a probability distribution and implements the temperature computation.\n",
        "    def inference(self, x, hidden_state=None, temperature=1):\n",
        "        x = x.view(-1, 1)\n",
        "        x, hidden_state = self.forward(x, hidden_state)\n",
        "        x = x.view(1, -1)\n",
        "        x = x / max(temperature, 1e-20)\n",
        "        x = F.softmax(x, dim=1)\n",
        "        return x, hidden_state\n",
        "\n",
        "    # Predefined loss function\n",
        "    def loss(self, prediction, label, reduction='mean'):\n",
        "        loss_val = F.cross_entropy(prediction.view(-1, self.vocab_size), label.view(-1), reduction=reduction)\n",
        "        return loss_val\n",
        "\n",
        "    # Saves the current model\n",
        "    def save_model(self, file_path, num_to_keep=1):\n",
        "        pt_util.save(self, file_path, num_to_keep)\n",
        "\n",
        "    # Saves the best model so far\n",
        "    def save_best_model(self, accuracy, file_path, num_to_keep=1):\n",
        "        if accuracy > self.best_accuracy:\n",
        "            self.save_model(file_path, num_to_keep)\n",
        "            self.best_accuracy = accuracy\n",
        "\n",
        "    def load_model(self, file_path):\n",
        "        pt_util.restore(self, file_path)\n",
        "\n",
        "    def load_last_model(self, dir_path):\n",
        "        return pt_util.restore_latest(self, dir_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qed4hrCrJEG0"
      },
      "outputs": [],
      "source": [
        "def generate_single_note_seed(vocab, seq_len=50):\n",
        "    seed = [vocab.ind2note[0] for _ in range(seq_len - 1)]\n",
        "    note = random.choice(list(vocab.note2ind.keys()))\n",
        "    while note == vocab.ind2note[0]:\n",
        "        note = random.choice([vocab.note2ind.keys()])\n",
        "    seed.append(note)\n",
        "    return seed\n",
        "\n",
        "\n",
        "def generate_song_start_seed(data_path, fs, seq_len=50):\n",
        "    if not isfile(data_path):\n",
        "        raise ValueError('Enter a valid midi file')\n",
        "    else:\n",
        "        midi_pretty_format = pretty_midi.PrettyMIDI(data_path)\n",
        "        piano_midi = midi_pretty_format.instruments[0]  # Get the piano channels\n",
        "        piano_roll = piano_midi.get_piano_roll(fs=fs)\n",
        "        cur_music = []\n",
        "        index = np.where(piano_roll > 0)\n",
        "        times = np.unique(index[1])\n",
        "        for time in times:\n",
        "            index_where = np.where(index[1] == time)\n",
        "            notes = index[0][index_where]\n",
        "            notes = tuple(notes)\n",
        "            cur_music.append(notes)\n",
        "            if len(cur_music) == seq_len:\n",
        "                return cur_music\n",
        "        raise Exception(\"Shouldn't get here\")\n",
        "\n",
        "def piano_roll_to_pretty_midi(piano_roll, fs, program=0):\n",
        "    '''Convert a Piano Roll array into a PrettyMidi object\n",
        "     with a single instrument.\n",
        "    Parameters\n",
        "    ----------\n",
        "    piano_roll : np.ndarray, shape=(128,frames), dtype=int\n",
        "        Piano roll of one instrument\n",
        "    fs : int\n",
        "        Sampling frequency of the columns, i.e. each column is spaced apart\n",
        "        by ``1./fs`` seconds.\n",
        "    program : int\n",
        "        The program number of the instrument.\n",
        "    Returns\n",
        "    -------\n",
        "    midi_object : pretty_midi.PrettyMIDI\n",
        "        A pretty_midi.PrettyMIDI class instance describing\n",
        "        the piano roll.\n",
        "    '''\n",
        "    notes, _ = piano_roll.shape\n",
        "    pm = pretty_midi.PrettyMIDI()\n",
        "    instrument = pretty_midi.Instrument(program=program)\n",
        "\n",
        "    # pad 1 column of zeros so we can acknowledge inital and ending events\n",
        "    piano_roll = np.pad(piano_roll, [(0, 0), (1, 1)], 'constant')\n",
        "\n",
        "    # use changes in velocities to find note on / note off events\n",
        "    velocity_changes = np.nonzero(np.diff(piano_roll).T)\n",
        "\n",
        "    # keep track on velocities and note on times\n",
        "    prev_velocities = np.zeros(notes, dtype=int)\n",
        "    note_on_time = np.zeros(notes)\n",
        "\n",
        "    for time, note in zip(*velocity_changes):\n",
        "        # use time + 1 because of padding above\n",
        "        velocity = piano_roll[note, time + 1]\n",
        "        time = time / fs\n",
        "        if velocity > 0:\n",
        "            if prev_velocities[note] == 0:\n",
        "                note_on_time[note] = time\n",
        "                prev_velocities[note] = velocity\n",
        "        else:\n",
        "            pm_note = pretty_midi.Note(\n",
        "                velocity=prev_velocities[note],\n",
        "                pitch=note,\n",
        "                start=note_on_time[note],\n",
        "                end=time)\n",
        "            instrument.notes.append(pm_note)\n",
        "            prev_velocities[note] = 0\n",
        "    pm.instruments.append(instrument)\n",
        "    return pm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "LG9UlSVOWnXE"
      },
      "outputs": [],
      "source": [
        "TEMPERATURE = 0.9\n",
        "BEAM_WIDTH = 10\n",
        "\n",
        "def max_sampling_strategy(sequence_length, model, output, hidden, vocab):\n",
        "    outputs = []\n",
        "    for _ in range(sequence_length):\n",
        "        max_idx = torch.argmax(output)\n",
        "        outputs.append(max_idx)\n",
        "        output, hidden = model.inference(max_idx, hidden, TEMPERATURE)\n",
        "        \n",
        "    return outputs\n",
        "    \n",
        "def sample_sampling_strategy(sequence_length, model, output, hidden, vocab):\n",
        "    outputs = []\n",
        "    for _ in range(sequence_length):\n",
        "        sample_idx = torch.multinomial(output, 1)\n",
        "        outputs.append(sample_idx)\n",
        "        output, hidden = model.inference(sample_idx, hidden, TEMPERATURE)\n",
        "\n",
        "    return outputs\n",
        "\n",
        "def beam_thread_task(beam, beam_width, model, new_beams, beam_lock):\n",
        "    sample_idxs = torch.multinomial(beam[1], beam_width, replacement=True)\n",
        "    for new_sample in torch.flatten(sample_idxs):\n",
        "        new_output, new_hidden = model.inference(new_sample, beam[2], TEMPERATURE)\n",
        "        new_outputs = beam[0].copy() # generated texts\n",
        "        new_outputs.append(new_sample)\n",
        "        beam_lock.acquire()\n",
        "        new_beams.append((new_outputs, new_output, new_hidden, \n",
        "                            beam[3] + torch.log(new_sample).item()))\n",
        "        beam_lock.release()\n",
        "\n",
        "def beam_sampling_strategy(sequence_length, beam_width, model, output, hidden, vocab):\n",
        "    beams = [([], output, hidden, 0)]\n",
        "    beam_lock = threading.Lock()\n",
        "    for _ in range(sequence_length):\n",
        "        new_beams = []\n",
        "        threads = [threading.Thread(target=beam_thread_task, args=(beam, beam_width, model, new_beams, beam_lock)) for beam in beams]\n",
        "        for thread in threads:\n",
        "            thread.start()\n",
        "        for thread in threads:\n",
        "            thread.join()\n",
        "        new_beams = sorted(new_beams, key=lambda x: x[3], reverse=True)\n",
        "        beams = new_beams[:beam_width]\n",
        "\n",
        "    return beams[0][0]\n",
        "\n",
        "def generate_language(model, device, seed_notes, vocab, midi_file_name, fs,\n",
        "                      start_index=49, max_generated=300, sampling_strategy='max', beam_width=BEAM_WIDTH):\n",
        "    model.eval()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        seed_notes_arr = vocab.notes_to_array(seed_notes)\n",
        "\n",
        "        # Computes the initial hidden state from the prompt (seed words).\n",
        "        hidden = None\n",
        "        for ind in seed_notes_arr:\n",
        "            data = ind.to(device)\n",
        "            output, hidden = model.inference(data, hidden)\n",
        "        # test_multinomial(1000, output)\n",
        "      \n",
        "        \n",
        "        if sampling_strategy == 'max':\n",
        "            outputs = max_sampling_strategy(max_generated, model, output, hidden, vocab)\n",
        "\n",
        "        elif sampling_strategy == 'sample':\n",
        "            outputs = sample_sampling_strategy(max_generated, model, output, hidden, vocab)\n",
        "\n",
        "        elif sampling_strategy == 'beam':\n",
        "            outputs = beam_sampling_strategy(max_generated, beam_width, model, output, hidden, vocab)\n",
        "\n",
        "\n",
        "        note_arr = vocab.array_to_notes(torch.cat((seed_notes_arr, torch.Tensor(outputs))))\n",
        "        array_piano_roll = np.zeros((128, max_generated + 1), dtype=np.int16) # max_generated is len(note_arr)\n",
        "        for index, note in enumerate(note_arr[start_index:]):\n",
        "            if note != vocab.ind2note[0]:\n",
        "                for j in note:\n",
        "                    array_piano_roll[int(j), index] = 1\n",
        "        generate_to_midi = piano_roll_to_pretty_midi(array_piano_roll, fs=fs)\n",
        "        # print(\"Tempo {}\".format(generate_to_midi.estimate_tempo()))\n",
        "        for note in generate_to_midi.instruments[0].notes:\n",
        "            note.velocity = 100\n",
        "        generate_to_midi.write(midi_file_name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CLF2Iqvoa8hS"
      },
      "outputs": [],
      "source": [
        "import tqdm\n",
        "def repackage_hidden(h):\n",
        "    \"\"\"Wraps hidden states in new Tensors, to detach them from their history.\"\"\"\n",
        "    if isinstance(h, torch.Tensor):\n",
        "        return h.detach()\n",
        "    else:\n",
        "        return tuple(repackage_hidden(v) for v in h)\n",
        "\n",
        "def train(model, device, optimizer, train_loader, lr, epoch, log_interval):\n",
        "    model.train()\n",
        "    losses = []\n",
        "    hidden = None\n",
        "    for batch_idx, (data, label) in enumerate(tqdm.tqdm(train_loader)):\n",
        "        data, label = data.to(device), label.to(device)\n",
        "        # Separates the hidden state across batches. \n",
        "        # Otherwise the backward would try to go all the way to the beginning every time.\n",
        "        if hidden is not None:\n",
        "            hidden = repackage_hidden(hidden)\n",
        "        optimizer.zero_grad()\n",
        "        output, hidden = model(data)\n",
        "        pred = output.max(-1)[1]\n",
        "        loss = model.loss(output, label)\n",
        "        losses.append(loss.item())\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        if batch_idx % log_interval == 0:\n",
        "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
        "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
        "                100. * batch_idx / len(train_loader), loss.item()))\n",
        "    return np.mean(losses)\n",
        "\n",
        "\n",
        "def test(model, device, test_loader):\n",
        "    model.eval()\n",
        "    test_loss = 0\n",
        "    correct = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        hidden = None\n",
        "        for batch_idx, (data, label) in enumerate(test_loader):\n",
        "            data, label = data.to(device), label.to(device)\n",
        "            output, hidden = model(data, hidden)\n",
        "            test_loss += model.loss(output, label, reduction='mean').item()\n",
        "            pred = output.max(-1)[1]\n",
        "            correct_mask = pred.eq(label.view_as(pred))\n",
        "            num_correct = correct_mask.sum().item()\n",
        "            correct += num_correct          \n",
        "\n",
        "    test_loss /= len(test_loader)\n",
        "    test_accuracy = 100. * correct / (len(test_loader.dataset) * test_loader.dataset.sequence_length)\n",
        "\n",
        "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
        "        test_loss, correct, len(test_loader.dataset) * test_loader.dataset.sequence_length,\n",
        "        100. * correct / (len(test_loader.dataset) * test_loader.dataset.sequence_length)))\n",
        "    return test_loss, test_accuracy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2YWgLaeZbD8Y"
      },
      "outputs": [],
      "source": [
        "SEED_SIZE = 50\n",
        "SEQUENCE_LENGTH = 100\n",
        "BATCH_SIZE = 256\n",
        "FEATURE_SIZE = 512\n",
        "TEST_BATCH_SIZE = 256\n",
        "EPOCHS = 200\n",
        "LEARNING_RATE = 0.002\n",
        "WEIGHT_DECAY = 0.0005\n",
        "USE_CUDA = True\n",
        "PRINT_INTERVAL = 10\n",
        "LOG_PATH = DATA_PATH + 'logs/log.pkl'\n",
        "DATASET_NAME = \"schumann_2\"\n",
        "\n",
        "def main():\n",
        "    data_train = PianoDataset(DATA_PATH + f\"music_train_{DATASET_NAME}.pkl\", SEQUENCE_LENGTH, BATCH_SIZE)\n",
        "    data_test = PianoDataset(DATA_PATH + f\"music_test_{DATASET_NAME}.pkl\", SEQUENCE_LENGTH, TEST_BATCH_SIZE)\n",
        "    vocab = data_train.vocab\n",
        "\n",
        "    use_cuda = USE_CUDA and torch.cuda.is_available()\n",
        "\n",
        "    device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
        "    print('Using device', device)\n",
        "    # import multiprocessing\n",
        "    # num_workers = multiprocessing.cpu_count()\n",
        "    num_workers = 0\n",
        "    print('num workers:', num_workers)\n",
        "\n",
        "    kwargs = {'num_workers': num_workers,\n",
        "              'pin_memory': True} if use_cuda else {}\n",
        "\n",
        "    train_loader = torch.utils.data.DataLoader(data_train, batch_size=BATCH_SIZE,\n",
        "                                               shuffle=False, **kwargs)\n",
        "    test_loader = torch.utils.data.DataLoader(data_test, batch_size=TEST_BATCH_SIZE,\n",
        "                                              shuffle=False, **kwargs)\n",
        "\n",
        "    model = PianoLSTMNet(data_train.vocab_size(), FEATURE_SIZE).to(device)\n",
        "\n",
        "    # Adam is an optimizer like SGD but a bit fancier. It tends to work faster and better than SGD.\n",
        "    # We will talk more about different optimization methods in class.\n",
        "    optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)\n",
        "    start_epoch = model.load_last_model(DATA_PATH + 'checkpoints')\n",
        "\n",
        "    train_losses, test_losses, test_accuracies = pt_util.read_log(LOG_PATH, ([], [], []))\n",
        "    test_loss, test_accuracy = test(model, device, test_loader)\n",
        "\n",
        "    test_losses.append((start_epoch, test_loss))\n",
        "    test_accuracies.append((start_epoch, test_accuracy))\n",
        "\n",
        "    try:\n",
        "        for epoch in range(start_epoch, EPOCHS + 1):\n",
        "            lr = LEARNING_RATE * np.power(0.25, (int(epoch / 6)))\n",
        "            train_loss = train(model, device, optimizer, train_loader, lr, epoch, PRINT_INTERVAL)\n",
        "            test_loss, test_accuracy = test(model, device, test_loader)\n",
        "            train_losses.append((epoch, train_loss))\n",
        "            test_losses.append((epoch, test_loss))\n",
        "            test_accuracies.append((epoch, test_accuracy))\n",
        "            pt_util.write_log(LOG_PATH, (train_losses, test_losses, test_accuracies))\n",
        "            model.save_best_model(test_accuracy, DATA_PATH + 'checkpoints/%03d.pt' % epoch)\n",
        "\n",
        "    except KeyboardInterrupt as ke:\n",
        "        print('Interrupted')\n",
        "    except:\n",
        "        import traceback\n",
        "        traceback.print_exc()\n",
        "    finally:\n",
        "        print('Saving final model')\n",
        "        model.save_model(DATA_PATH + 'checkpoints/%03d.pt' % epoch, 0)\n",
        "        ep, val = zip(*train_losses)\n",
        "        pt_util.plot(ep, val, 'Train loss', 'Epoch', 'Error')\n",
        "        ep, val = zip(*train_losses)\n",
        "        pt_util.plot(ep, np.exp(val), 'Train Perplexity', 'Epoch', 'Error')\n",
        "        ep, val = zip(*test_losses)\n",
        "        pt_util.plot(ep, val, 'Test loss', 'Epoch', 'Error')\n",
        "        ep, val = zip(*test_losses)\n",
        "        pt_util.plot(ep, np.exp(val), 'Test Perplexity', 'Epoch', 'Error')\n",
        "        ep, val = zip(*test_accuracies)\n",
        "        pt_util.plot(ep, val, 'Test accuracy', 'Epoch', 'Error')\n",
        "        return model, vocab, device\n",
        "\n",
        "final_model, vocab, device = main()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_X1tEJyFbMF1"
      },
      "outputs": [],
      "source": [
        "TITLE = \"schumann_lstm_dataset_fsin_2_fsout_8_temp_0p9\"\n",
        "\n",
        "def eval_final_model(model, vocab, device, title, fs):\n",
        "    seed_notes = generate_single_note_seed(vocab, SEED_SIZE)\n",
        "    \n",
        "    generate_language(model, device, seed_notes, vocab, f\"max_sample_{title}.mid\", fs=fs)\n",
        "    print('generated with max')\n",
        "\n",
        "    for ii in range(5):\n",
        "        generate_language(model, device, seed_notes, vocab, f\"sampling_sample_{ii}_{title}.mid\", sampling_strategy='sample', fs=fs)\n",
        "        print('generated with sample')\n",
        "\n",
        "    for ii in range(5):\n",
        "        generate_language(model, device, seed_notes, vocab, f\"beam_sample_{ii}_{title}.mid\", sampling_strategy='beam', fs=fs)\n",
        "        print('generated with beam')\n",
        "\n",
        "eval_final_model(final_model, vocab, device, TITLE, 8)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Restoring:\n",
            "encoder.weight -> \ttorch.Size([42436, 512]) = 86MB\n",
            "lstm.weight_ih_l0 -> \ttorch.Size([2048, 512]) = 4MB\n",
            "lstm.weight_hh_l0 -> \ttorch.Size([2048, 512]) = 4MB\n",
            "lstm.bias_ih_l0 -> \ttorch.Size([2048]) = 0MB\n",
            "lstm.bias_hh_l0 -> \ttorch.Size([2048]) = 0MB\n",
            "lstm.weight_ih_l1 -> \ttorch.Size([2048, 512]) = 4MB\n",
            "lstm.weight_hh_l1 -> \ttorch.Size([2048, 512]) = 4MB\n",
            "lstm.bias_ih_l1 -> \ttorch.Size([2048]) = 0MB\n",
            "lstm.bias_hh_l1 -> \ttorch.Size([2048]) = 0MB\n",
            "decoder.weight -> \ttorch.Size([42436, 512]) = 86MB\n",
            "decoder.bias -> \ttorch.Size([42436]) = 0MB\n",
            "\n",
            "Restored all variables\n",
            "No new variables\n",
            "Restored data\\checkpoints\\088.pt\n",
            "generated with max\n",
            "generated with sample\n",
            "generated with sample\n",
            "generated with sample\n",
            "generated with sample\n",
            "generated with sample\n",
            "generated with beam\n",
            "generated with beam\n",
            "generated with beam\n",
            "generated with beam\n",
            "generated with beam\n"
          ]
        }
      ],
      "source": [
        "last_model = PianoLSTMNet(len(vocab), FEATURE_SIZE).to(device)\n",
        "last_model.load_last_model(DATA_PATH + 'checkpoints')\n",
        "eval_final_model(last_model, vocab, device, \"schumann_latest_lstm_dataset_fsin_2_fsout_8_temp_0p9\", 8)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "generated with max\n",
            "generated with sample\n",
            "generated with sample\n",
            "generated with sample\n",
            "generated with sample\n",
            "generated with sample\n",
            "generated with beam\n",
            "generated with beam\n",
            "generated with beam\n",
            "generated with beam\n",
            "generated with beam\n"
          ]
        }
      ],
      "source": [
        "eval_final_model(last_model, vocab, device, \"maestro_latest_dataset_fsin_2_fsout_2_temp_0p9\", 2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "generated with max\n",
            "generated with sample\n",
            "generated with sample\n",
            "generated with sample\n",
            "generated with sample\n",
            "generated with sample\n",
            "generated with beam\n",
            "generated with beam\n",
            "generated with beam\n",
            "generated with beam\n",
            "generated with beam\n"
          ]
        }
      ],
      "source": [
        "eval_final_model(final_model, vocab, device, \"maestro_dataset_fsin_2_fsout_2_temp_0p9\", 2)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "deeplearning",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.15 (default, Nov 24 2022, 09:04:07) \n[Clang 14.0.6 ]"
    },
    "vscode": {
      "interpreter": {
        "hash": "51432b8e5767c06330d9b51dfad63f9db0ea39868e37d921b9c2e277373f8d11"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
